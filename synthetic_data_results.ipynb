{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(16,10)}, font_scale=1.3)\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.tree.branchings import maximum_branching\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_confounders_2011 = pd.read_csv('processed_data/full_dataset_confounders_2011.csv')\n",
    "full_dataset_confounders_2012 = pd.read_csv('processed_data/full_dataset_confounders_2012.csv')\n",
    "full_dataset_confounders_2013 = pd.read_csv('processed_data/full_dataset_confounders_2013.csv')\n",
    "\n",
    "full_dataset_confounders_2011['Aged 16-64 Percentage'] = full_dataset_confounders_2011[\"('Population and Age; Aged 16-64; 2011', '2011')\"] / (full_dataset_confounders_2011[\"('Population and Age; Aged 16-64; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Population and Age; Aged 0-15; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Population and Age; Aged 65+; 2011', '2011')\"]) * 100\n",
    "full_dataset_confounders_2011['Aged 0-15 Percentage'] = full_dataset_confounders_2011[\"('Population and Age; Aged 0-15; 2011', '2011')\"] / (full_dataset_confounders_2011[\"('Population and Age; Aged 16-64; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Population and Age; Aged 0-15; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Population and Age; Aged 65+; 2011', '2011')\"]) * 100\n",
    "full_dataset_confounders_2011['Aged 65+ Percentage'] = full_dataset_confounders_2011[\"('Population and Age; Aged 65+; 2011', '2011')\"] / (full_dataset_confounders_2011[\"('Population and Age; Aged 16-64; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Population and Age; Aged 0-15; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Population and Age; Aged 65+; 2011', '2011')\"]) * 100\n",
    "full_dataset_confounders_2011[\"Workplace employment; Number of Full-time employees\"] = full_dataset_confounders_2011[\"('Workplace employment; Number of Full-time employees; 2011', '2011')\"] / (full_dataset_confounders_2011[\"('Workplace employment; Number of Part-time employees; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Workplace employment; Number of Full-time employees; 2011', '2011')\"]) * 100\n",
    "full_dataset_confounders_2011[\"Workplace employment; Number of Part-time employees\"] = full_dataset_confounders_2011[\"('Workplace employment; Number of Part-time employees; 2011', '2011')\"] / (full_dataset_confounders_2011[\"('Workplace employment; Number of Part-time employees; 2011', '2011')\"] + full_dataset_confounders_2011[\"('Workplace employment; Number of Full-time employees; 2011', '2011')\"]) * 100\n",
    "full_dataset_confounders_2011 = full_dataset_confounders_2011.rename(columns={\"('Access to green space and nature; % area that is greenspace; 2012', '2012')\": \"Area that is greenspace\",  \"('Access to green space and nature; % homes with deficiency in access to nature; 2012', '2012')\": \"Percentage homes with deficiency in access to nature\", \n",
    "    \"('Benefits claimants; DWP Working-age client group (rates); 2011', '2011')\":  \"DWP Working-age client group (rates)\", \"('Benefits claimants; Employment and support allowance claimants; 2011', '2011')\": \"Employment and support allowance claimants\", \"('Benefits claimants; Housing Benefit rates; 2011', '2011')\": \"Housing Benefit rates\",\n",
    "    \"('Benefits claimants; Income Support Claimants; 2011', '2011')\":\"Income Support Claimants\", \"('Benefits claimants; Incapacity Benefit Claimants; 2011', '2011')\": \"Incapacity Benefit Claimants\", \"('Jobseekers Allowance; JSA Claimant Rate; 2011', '2011')\": \"JSA Claimant Rate\"})\n",
    "\n",
    "\n",
    "full_dataset_confounders_2012['Aged 16-64 Percentage'] = full_dataset_confounders_2012[\"('Population and Age; Aged 16-64; 2012', '2012')\"] / (full_dataset_confounders_2012[\"('Population and Age; Aged 16-64; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Population and Age; Aged 0-15; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Population and Age; Aged 65+; 2012', '2012')\"]) * 100\n",
    "full_dataset_confounders_2012['Aged 0-15 Percentage'] = full_dataset_confounders_2012[\"('Population and Age; Aged 0-15; 2012', '2012')\"] / (full_dataset_confounders_2012[\"('Population and Age; Aged 16-64; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Population and Age; Aged 0-15; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Population and Age; Aged 65+; 2012', '2012')\"]) * 100\n",
    "full_dataset_confounders_2012['Aged 65+ Percentage'] = full_dataset_confounders_2012[\"('Population and Age; Aged 65+; 2012', '2012')\"] / (full_dataset_confounders_2012[\"('Population and Age; Aged 16-64; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Population and Age; Aged 0-15; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Population and Age; Aged 65+; 2012', '2012')\"]) * 100\n",
    "full_dataset_confounders_2012[\"Workplace employment; Number of Full-time employees\"] = full_dataset_confounders_2012[\"('Workplace employment; Number of Full-time employees; 2012', '2012')\"] / (full_dataset_confounders_2012[\"('Workplace employment; Number of Part-time employees; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Workplace employment; Number of Full-time employees; 2012', '2012')\"]) * 100\n",
    "full_dataset_confounders_2012[\"Workplace employment; Number of Part-time employees\"] = full_dataset_confounders_2012[\"('Workplace employment; Number of Part-time employees; 2012', '2012')\"] / (full_dataset_confounders_2012[\"('Workplace employment; Number of Part-time employees; 2012', '2012')\"] + full_dataset_confounders_2012[\"('Workplace employment; Number of Full-time employees; 2012', '2012')\"]) * 100\n",
    "full_dataset_confounders_2012 = full_dataset_confounders_2012.rename(columns={\"('Access to green space and nature; % area that is greenspace; 2012', '2012')\": \"Area that is greenspace\",  \"('Access to green space and nature; % homes with deficiency in access to nature; 2012', '2012')\": \"Percentage homes with deficiency in access to nature\", \n",
    "    \"('Benefits claimants; DWP Working-age client group (rates); 2012', '2012')\":  \"DWP Working-age client group (rates)\", \"('Benefits claimants; Employment and support allowance claimants; 2012', '2012')\": \"Employment and support allowance claimants\", \"('Benefits claimants; Housing Benefit rates; 2012', '2012')\": \"Housing Benefit rates\",\n",
    "    \"('Benefits claimants; Income Support Claimants; 2012', '2012')\":\"Income Support Claimants\", \"('Benefits claimants; Incapacity Benefit Claimants; 2012', '2012')\": \"Incapacity Benefit Claimants\", \"('Jobseekers Allowance; JSA Claimant Rate; 2012', '2012')\": \"JSA Claimant Rate\"})\n",
    "\n",
    "full_dataset_confounders_2013['Aged 16-64 Percentage'] = full_dataset_confounders_2013[\"('Population and Age; Aged 16-64; 2013', '2013')\"] / (full_dataset_confounders_2013[\"('Population and Age; Aged 16-64; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Population and Age; Aged 0-15; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Population and Age; Aged 65+; 2013', '2013')\"]) * 100\n",
    "full_dataset_confounders_2013['Aged 0-15 Percentage'] = full_dataset_confounders_2013[\"('Population and Age; Aged 0-15; 2013', '2013')\"] / (full_dataset_confounders_2013[\"('Population and Age; Aged 16-64; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Population and Age; Aged 0-15; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Population and Age; Aged 65+; 2013', '2013')\"]) * 100\n",
    "full_dataset_confounders_2013['Aged 65+ Percentage'] = full_dataset_confounders_2013[\"('Population and Age; Aged 65+; 2013', '2013')\"] / (full_dataset_confounders_2013[\"('Population and Age; Aged 16-64; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Population and Age; Aged 0-15; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Population and Age; Aged 65+; 2013', '2013')\"]) * 100\n",
    "full_dataset_confounders_2013[\"Workplace employment; Number of Full-time employees\"] = full_dataset_confounders_2013[\"('Workplace employment; Number of Full-time employees; 2013', '2013')\"] / (full_dataset_confounders_2013[\"('Workplace employment; Number of Part-time employees; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Workplace employment; Number of Full-time employees; 2013', '2013')\"]) * 100\n",
    "full_dataset_confounders_2013[\"Workplace employment; Number of Part-time employees\"] = full_dataset_confounders_2013[\"('Workplace employment; Number of Part-time employees; 2013', '2013')\"] / (full_dataset_confounders_2013[\"('Workplace employment; Number of Part-time employees; 2013', '2013')\"] + full_dataset_confounders_2013[\"('Workplace employment; Number of Full-time employees; 2013', '2013')\"]) * 100\n",
    "full_dataset_confounders_2013 = full_dataset_confounders_2013.rename(columns={\"('Access to green space and nature; % area that is greenspace; 2012', '2012')\": \"Area that is greenspace\",  \"('Access to green space and nature; % homes with deficiency in access to nature; 2012', '2012')\": \"Percentage homes with deficiency in access to nature\", \n",
    "    \"('Benefits claimants; DWP Working-age client group (rates); 2013', '2013')\":  \"DWP Working-age client group (rates)\", \"('Benefits claimants; Employment and support allowance claimants; 2013', '2013')\": \"Employment and support allowance claimants\", \"('Benefits claimants; Housing Benefit rates; 2013', '2013')\": \"Housing Benefit rates\",\n",
    "    \"('Benefits claimants; Income Support Claimants; 2013', '2013')\":\"Income Support Claimants\", \"('Benefits claimants; Incapacity Benefit Claimants; 2013', '2013')\": \"Incapacity Benefit Claimants\", \"('Jobseekers Allowance; JSA Claimant Rate; 2013', '2013')\": \"JSA Claimant Rate\"})\n",
    "\n",
    "\n",
    "full_dataset_confounders_2011.drop([\"('Population and Age; Aged 16-64; 2011', '2011')\", \"('Population and Age; Aged 0-15; 2011', '2011')\", \"('Population and Age; Aged 65+; 2011', '2011')\", \"('Workplace employment; Number of Full-time employees; 2011', '2011')\", \"('Workplace employment; Number of Part-time employees; 2011', '2011')\"], axis=1, inplace=True)\n",
    "full_dataset_confounders_2012.drop([\"('Population and Age; Aged 16-64; 2012', '2012')\", \"('Population and Age; Aged 0-15; 2012', '2012')\", \"('Population and Age; Aged 65+; 2012', '2012')\", \"('Workplace employment; Number of Full-time employees; 2012', '2012')\", \"('Workplace employment; Number of Part-time employees; 2012', '2012')\"], axis=1, inplace=True)\n",
    "full_dataset_confounders_2013.drop([\"('Population and Age; Aged 16-64; 2013', '2013')\", \"('Population and Age; Aged 0-15; 2013', '2013')\", \"('Population and Age; Aged 65+; 2013', '2013')\", \"('Workplace employment; Number of Full-time employees; 2013', '2013')\", \"('Workplace employment; Number of Part-time employees; 2013', '2013')\"], axis=1, inplace=True)\n",
    "\n",
    "full_dataset_confounders_2011.fillna(0, inplace=True)\n",
    "full_dataset_confounders_2012.fillna(0, inplace=True)\n",
    "full_dataset_confounders_2013.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get same results as in results2\n",
    "\n",
    "def get_treatment_level(input, bins):\n",
    "    if input < bins[0]:\n",
    "        return 0\n",
    "    elif input < bins[1]:\n",
    "        return 1\n",
    "    elif input < bins[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def logit(p):\n",
    "    logit_value = math.log(p / (1-p))\n",
    "    return logit_value\n",
    "\n",
    "def plot_propensity_plots(predictions, predictions_logit, T):\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.suptitle('Density distribution plots for propensity score and logit(propensity score).')\n",
    "    sns.kdeplot(x = predictions[:,1], hue = T , ax = ax[0])\n",
    "    ax[0].set_title('Propensity Score')\n",
    "    sns.kdeplot(x = predictions_logit, hue = T , ax = ax[1])\n",
    "    ax[1].axvline(-0.4, ls='--')\n",
    "    ax[1].set_title('Logit of Propensity Score')\n",
    "    plt.show()\n",
    "\n",
    "def create_maximum_branching_graph(df_data):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df_data.reset_index()['osward'].tolist())\n",
    "    df_data_no_index = df_data.reset_index()\n",
    "    epsilon = 0.0001\n",
    "    for index, row in df_data_no_index.iterrows():\n",
    "        other_rows = df_data_no_index[df_data_no_index['treatment'] != row.treatment]\n",
    "        for o_index, o_row in other_rows.reset_index().iterrows():\n",
    "            if not G.has_edge(row.osward, o_row.osward):\n",
    "                modified_distance = (abs(row.propensity_score_logit - o_row.propensity_score_logit) + epsilon) / abs(row.treatment - o_row.treatment)\n",
    "                G.add_edge(row.osward, o_row.osward, weight=modified_distance)\n",
    "    return maximum_branching(G)\n",
    "\n",
    "def plot_correlation(input_data, treatment_column):\n",
    "    correlation = input_data[[treatment_column, 'outcome']]\n",
    "    correlation.plot.scatter(x=treatment_column, y='outcome')\n",
    "    spear_corr = stats.spearmanr(list(correlation[treatment_column]), list(correlation['outcome']))\n",
    "    print('Spearman correlation:', spear_corr.correlation, 'p-value:', spear_corr.pvalue)\n",
    "\n",
    "def plot_distance_distribution(edmonds_applied, df_data):\n",
    "    distances = []\n",
    "    treatment_effect = []\n",
    "    for (u,v) in edmonds_applied.edges():\n",
    "        distances.append(abs(df_data['treatment'][u] - df_data['treatment'][v]))\n",
    "        effect = (df_data['outcome'][u] - df_data['outcome'][v])/(df_data['treatment'][u] - df_data['treatment'][v])\n",
    "        treatment_effect.append(effect)\n",
    "    distribution_data = pd.DataFrame(list(zip(distances,treatment_effect)), columns = ['Distance', 'Effect'])\n",
    "    sns.jointplot(distribution_data, x='Distance', y='Effect', kind='kde', fill=True)\n",
    "\n",
    "def print_distance_confusion_matrix(edmonds_applied, df_data):\n",
    "    low_dose_units = []\n",
    "    high_dose_units = []\n",
    "\n",
    "    for (u,v) in edmonds_applied.edges():\n",
    "        u_treatment = df_data['treatment'][u]\n",
    "        v_treatment = df_data['treatment'][v]\n",
    "        if u_treatment > v_treatment:\n",
    "            low_dose_units.append(v_treatment)\n",
    "            high_dose_units.append(u_treatment)\n",
    "            continue\n",
    "        low_dose_units.append(u_treatment)\n",
    "        high_dose_units.append(v_treatment)\n",
    "\n",
    "    print(confusion_matrix(high_dose_units, low_dose_units))\n",
    "\n",
    "def obtain_results(input_data, treatment_column, outcome_column, columns_to_drop, bins):\n",
    "    df = input_data.copy()\n",
    "    subset = df.drop(columns_to_drop, axis=1)\n",
    "    df[treatment_column] = df[treatment_column].astype('int')\n",
    "    subset[treatment_column] = subset[treatment_column].astype('int')\n",
    "    treatment_levels = [get_treatment_level(x, bins) for x in list(df[treatment_column])]\n",
    "    subset_treatment_levels = [get_treatment_level(x, bins) for x in list(subset[treatment_column])]\n",
    "    df[treatment_column + '_bin'] = treatment_levels\n",
    "    subset[treatment_column] = subset_treatment_levels\n",
    "\n",
    "    T = subset[treatment_column]\n",
    "    X = subset.loc[:,subset.columns != treatment_column]\n",
    "    y = input_data[[outcome_column]]\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logistic_classifier', lr())\n",
    "    ])\n",
    "    pipe.fit(X, T)\n",
    "\n",
    "    predictions = pipe.predict_proba(X)\n",
    "    predictions_binary = pipe.predict(X)\n",
    "    # print('Accuracy: {:.4f}\\n'.format(metrics.accuracy_score(T, predictions_binary)))\n",
    "    # print('Confusion matrix:\\n{}\\n'.format(metrics.confusion_matrix(T, predictions_binary)))\n",
    "    # print('F1 score is:', metrics.f1_score(T, predictions_binary, average=None))\n",
    "    predictions_logit = np.array([logit(xi) for xi in predictions[:,1]])\n",
    "    # plot_propensity_plots(predictions, predictions_logit, T)\n",
    "    df.loc[:,'propensity_score'] = predictions[:,1]\n",
    "    df.loc[:,'propensity_score_logit'] = predictions_logit\n",
    "    df.loc[:,'outcome'] = y[outcome_column]\n",
    "\n",
    "    X.loc[:,'propensity_score'] = predictions[:,1]\n",
    "    X.loc[:,'propensity_score_logit'] = predictions_logit\n",
    "    X.loc[:,'outcome'] = y[outcome_column]\n",
    "    X.loc[:,'treatment'] = df[treatment_column + '_bin']\n",
    "\n",
    "    caliper = np.std(df.propensity_score) * 0.25\n",
    "    # print('\\nCaliper (radius) is: {:.4f}\\n'.format(caliper))\n",
    "\n",
    "    df_data = X\n",
    "    knn = NearestNeighbors(n_neighbors=10 , p = 2, radius=caliper)\n",
    "    knn.fit(df_data[['propensity_score_logit']].to_numpy())\n",
    "    \n",
    "    distances , indexes = knn.kneighbors(\n",
    "        df_data[['propensity_score_logit']].to_numpy(), \\\n",
    "        n_neighbors=10)\n",
    "    df_data['osward'] = range(1, len(df_data) + 1)\n",
    "    df_data.osward = 'E' + df_data.osward.astype(str)\n",
    "    df_data.set_index('osward', inplace=True)\n",
    "\n",
    "    edmonds_applied = create_maximum_branching_graph(df_data)\n",
    "    treatment_effect = []\n",
    "    for (u,v) in edmonds_applied.edges():\n",
    "        # print(df_data['outcome'])\n",
    "        effect = (df_data['outcome'][u] - df_data['outcome'][v])/(df_data['treatment'][u] - df_data['treatment'][v])\n",
    "        treatment_effect.append(effect)\n",
    "    return treatment_effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truncnorm_generated(dataset, size):\n",
    "    a = dataset.min()\n",
    "    b = dataset.max()\n",
    "    mu = dataset.mean()\n",
    "    sigma = dataset.std()\n",
    "    return stats.truncnorm.rvs(a, b, loc=mu, scale=sigma, size=size)\n",
    "\n",
    "def get_exponential_generated(dataset, size):\n",
    "    mu = dataset.mean()\n",
    "    sigma = dataset.std()\n",
    "    return stats.expon.rvs(loc=mu, scale=sigma, size=size)\n",
    "\n",
    "def get_exponential_treatment_generated(row, a = 200):\n",
    "    scale = row.sum() / a\n",
    "    return stats.expon.rvs(loc = 0, scale=scale, size=1)[0]\n",
    "\n",
    "def get_truncated_outcome_generated(row, confounders, a = 0.000015, b = 100, ate = -1):\n",
    "    alpha = a * confounders.sum()\n",
    "    beta = b * confounders.sum()\n",
    "    mu = confounders.mean()\n",
    "    sigma = confounders.std()\n",
    "    return max(stats.truncnorm.rvs(a = alpha, b = beta, loc = mu, scale = sigma, size=1)[0] + ate * row['treatment'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_averages(effects, amt):\n",
    "    ate = 0\n",
    "    minimum = 0\n",
    "    percentile_25 = 0\n",
    "    percentile_75 = 0\n",
    "    maximum = 0\n",
    "    for eff in effects:\n",
    "        ate += np.mean(eff)\n",
    "        minimum += np.min(eff)\n",
    "        percentile_25 += np.percentile(eff,25)\n",
    "        percentile_75 += np.percentile(eff,75)\n",
    "        maximum += np.max(eff)\n",
    "    print('ATE:', ate/amt, 'Min:', minimum/amt,'25th %:', percentile_25/amt, '75th %:', percentile_75/amt, 'Max:', maximum/amt)\n",
    "\n",
    "\n",
    "def get_average_treatment_over_runs(dataset, ate= -1, runs=10):\n",
    "    simulated_dfs = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        age1664_generated = np.array(get_truncnorm_generated(dataset['Aged 16-64 Percentage'], 625))\n",
    "        age015_generated = np.array(get_truncnorm_generated(dataset['Aged 0-15 Percentage'], 625))\n",
    "        age65plus_generated = np.array(get_truncnorm_generated(dataset['Aged 65+ Percentage'], 625))\n",
    "        percentage_greenspace_generated = np.array(get_truncnorm_generated(dataset[\"Area that is greenspace\"], 625))\n",
    "        percentage_working_age_rates = np.array(get_truncnorm_generated(dataset[\"DWP Working-age client group (rates)\"], 625))\n",
    "        employment_support_claimants = np.array(get_truncnorm_generated(dataset[\"Employment and support allowance claimants\"], 625))\n",
    "        housing_benefit_rates = np.array(get_truncnorm_generated(dataset[\"Housing Benefit rates\"], 625))\n",
    "        income_support_claimants = np.array(get_truncnorm_generated(dataset[\"Income Support Claimants\"], 625))\n",
    "        incapacity_benefit_claimants = np.array(get_truncnorm_generated(dataset[\"Incapacity Benefit Claimants\"], 625))\n",
    "        jsa_claimant_rate = np.array(get_truncnorm_generated(dataset[\"JSA Claimant Rate\"], 625))\n",
    "\n",
    "        full_time_employees = np.array(get_exponential_generated(dataset[\"Workplace employment; Number of Full-time employees\"], 625))\n",
    "        part_time_employees = np.array(get_exponential_generated(dataset[\"Workplace employment; Number of Part-time employees\"], 625))\n",
    "        deficiency_access_nature = np.array(get_exponential_generated(dataset[\"Percentage homes with deficiency in access to nature\"], 625))\n",
    "        \n",
    "        simulated_confounders = [age1664_generated, age015_generated, age65plus_generated, percentage_greenspace_generated, percentage_working_age_rates, employment_support_claimants, housing_benefit_rates, income_support_claimants, incapacity_benefit_claimants, jsa_claimant_rate, full_time_employees, part_time_employees, deficiency_access_nature]\n",
    "        simulated_confounders_df = pd.DataFrame(simulated_confounders).transpose()\n",
    "        confounders = ['Aged 16-64 Percentage', 'Aged 0-15 Percentage', 'Aged 65+ Percentage', \"Area that is greenspace\", \"DWP Working-age client group (rates)\", \"Employment and support allowance claimants\", \"Housing Benefit rates\", \"Income Support Claimants\",\n",
    "            \"Incapacity Benefit Claimants\", \"JSA Claimant Rate\", \"Workplace employment; Number of Part-time employees\", \"Workplace employment; Number of Full-time employees\", \"Percentage homes with deficiency in access to nature\"]\n",
    "        simulated_confounders_df.columns = confounders\n",
    "        simulated_confounders_df['treatment'] = simulated_confounders_df.apply(lambda row: get_exponential_treatment_generated(row), axis=1)\n",
    "        simulated_confounders_df['outcome'] = simulated_confounders_df.apply(lambda row: get_truncated_outcome_generated(row, row[confounders], ate=ate), axis=1)\n",
    "        simulated_dfs.append(simulated_confounders_df)\n",
    "    effects = []\n",
    "    for simulated_df in simulated_dfs:\n",
    "        effects.append(obtain_results(simulated_df, 'treatment', 'outcome', ['outcome'], bins=[4,7,12]))\n",
    "    calculate_averages(effects, runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: -41.77904937989863 Min: -593.3413884368027 25th %: -132.7242385049974 75th %: 45.67480005249019 Max: 497.4065554285309\n"
     ]
    }
   ],
   "source": [
    "get_average_treatment_over_runs(full_dataset_confounders_2012, ate=-10, runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual facility count test\n",
    "\n",
    "\n",
    "arts_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['arts_count'], 10), 625)\n",
    "cinema_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['cinema_count'], 10), 625)\n",
    "gallery_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['gallery_count'], 10), 625)\n",
    "comm_center_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['comm_center_count'], 10), 625)\n",
    "dance_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['dance_count'], 10), 625)\n",
    "lgbt_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['lgbt_count'], 10), 625)\n",
    "library_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['library_count'], 10), 625)\n",
    "museum_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['museum_count'], 10), 625)\n",
    "music_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['music_count'], 10), 625)\n",
    "outdoor_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['outdoor_count'], 10), 625)\n",
    "pub_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['pub_count'], 10), 625)\n",
    "skate_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['skate_count'], 10), 625)\n",
    "theatre_count_2011 = np.repeat(get_exponential_generated(full_dataset_confounders_2011['theatre_count'], 10), 625)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

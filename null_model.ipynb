{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(16,10)}, font_scale=1.3)\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.tree.branchings import maximum_branching\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_treatment_level(input, bins):\n",
    "    if input < bins[0]:\n",
    "        return 0\n",
    "    elif input < bins[1]:\n",
    "        return 1\n",
    "    elif input < bins[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def logit(p):\n",
    "    logit_value = math.log(p / (1-p))\n",
    "    return logit_value\n",
    "\n",
    "def plot_propensity_plots(predictions, predictions_logit, T):\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.suptitle('Density distribution plots for propensity score and logit(propensity score).')\n",
    "    sns.kdeplot(x = predictions[:,1], hue = T , ax = ax[0])\n",
    "    ax[0].set_title('Propensity Score')\n",
    "    sns.kdeplot(x = predictions_logit, hue = T , ax = ax[1])\n",
    "    ax[1].axvline(-0.4, ls='--')\n",
    "    ax[1].set_title('Logit of Propensity Score')\n",
    "    plt.show()\n",
    "\n",
    "def create_maximum_branching_graph(df_data):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df_data.reset_index()['osward'].tolist())\n",
    "    df_data_no_index = df_data.reset_index()\n",
    "    epsilon = 0.0001\n",
    "    for index, row in df_data_no_index.iterrows():\n",
    "        other_rows = df_data_no_index[df_data_no_index['treatment'] != row.treatment]\n",
    "        for o_index, o_row in other_rows.reset_index().iterrows():\n",
    "            modified_distance = (abs(row.propensity_score_logit - o_row.propensity_score_logit) + epsilon) / abs(row.treatment - o_row.treatment)\n",
    "            if not G.has_edge(row.osward, o_row.osward):\n",
    "                G.add_edge(row.osward, o_row.osward, weight=modified_distance)\n",
    "    return maximum_branching(G)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_confounders_2011 = pd.read_csv('processed_data/full_dataset_confounders_2011.csv')\n",
    "full_dataset_confounders_2012 = pd.read_csv('processed_data/full_dataset_confounders_2012.csv')\n",
    "full_dataset_confounders_2013 = pd.read_csv('processed_data/full_dataset_confounders_2013.csv')\n",
    "\n",
    "full_dataset_confounders_2011.set_index('osward', inplace=True)\n",
    "full_dataset_confounders_2012.set_index('osward', inplace=True)\n",
    "full_dataset_confounders_2013.set_index('osward', inplace=True)\n",
    "\n",
    "all_columns_to_drop = ['arts_count', 'cinema_count', 'gallery_count',\n",
    "'comm_center_count', 'dance_count', 'lgbt_count', 'library_count', 'museum_count', 'music_count', 'outdoor_count',\n",
    "'pub_count', 'skate_count', 'theatre_count', 'total_count', 'QUANTITY']\n",
    "all_columns_to_drop_year = [x for x in all_columns_to_drop if x != \"total_count\"]\n",
    "all_columns_to_drop_arts = [x for x in all_columns_to_drop if x != \"arts_count\"]\n",
    "all_columns_to_drop_cinema = [x for x in all_columns_to_drop if x != \"cinema_count\"]\n",
    "all_columns_to_drop_gallery = [x for x in all_columns_to_drop if x != \"gallery_count\"]\n",
    "all_columns_to_drop_comm_center = [x for x in all_columns_to_drop if x != \"comm_center_count\"]\n",
    "all_columns_to_drop_dance = [x for x in all_columns_to_drop if x != \"dance_count\"]\n",
    "all_columns_to_drop_lgbt = [x for x in all_columns_to_drop if x != \"lgbt_count\"]\n",
    "all_columns_to_drop_library = [x for x in all_columns_to_drop if x != \"library_count\"]\n",
    "all_columns_to_drop_museum = [x for x in all_columns_to_drop if x != \"museum_count\"]\n",
    "all_columns_to_drop_music = [x for x in all_columns_to_drop if x != \"music_count\"]\n",
    "all_columns_to_drop_outdoor = [x for x in all_columns_to_drop if x != \"outdoor_count\"]\n",
    "all_columns_to_drop_pub = [x for x in all_columns_to_drop if x != \"pub_count\"]\n",
    "all_columns_to_drop_skate = [x for x in all_columns_to_drop if x != \"skate_count\"]\n",
    "all_columns_to_drop_theatre = [x for x in all_columns_to_drop if x != \"theatre_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_model(input_data, treatment_column, columns_to_drop, bins, iterations):\n",
    "    effects = []\n",
    "    for i in range(iterations):\n",
    "        df = input_data.copy()\n",
    "        subset = df.drop(columns_to_drop, axis=1)\n",
    "        df[treatment_column] = np.random.permutation(df[treatment_column].astype('int'))\n",
    "        subset[treatment_column] = subset[treatment_column].astype('int')\n",
    "        treatment_levels = [get_treatment_level(x, bins) for x in list(df[treatment_column])]\n",
    "        subset_treatment_levels = [get_treatment_level(x, bins) for x in list(subset[treatment_column])]\n",
    "        df[treatment_column + '_bin'] = treatment_levels\n",
    "        subset[treatment_column] = subset_treatment_levels\n",
    "\n",
    "        T = subset[treatment_column]\n",
    "        X = subset.loc[:,subset.columns != treatment_column]\n",
    "        y = df[['QUANTITY']]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('logistic_classifier', lr())\n",
    "        ])\n",
    "        pipe.fit(X, T)\n",
    "\n",
    "        predictions = pipe.predict_proba(X)\n",
    "        predictions_binary = pipe.predict(X)\n",
    "        predictions_logit = np.array([logit(xi) for xi in predictions[:,1]])\n",
    "        # plot_propensity_plots(predictions, predictions_logit, T)\n",
    "        df.loc[:,'propensity_score'] = predictions[:,1]\n",
    "        df.loc[:,'propensity_score_logit'] = predictions_logit\n",
    "        df.loc[:,'outcome'] = y.QUANTITY\n",
    "\n",
    "        X.loc[:,'propensity_score'] = predictions[:,1]\n",
    "        X.loc[:,'propensity_score_logit'] = predictions_logit\n",
    "        X.loc[:,'outcome'] = y.QUANTITY\n",
    "        X.loc[:,'treatment'] = df[treatment_column + '_bin']\n",
    "\n",
    "        caliper = np.std(df.propensity_score) * 0.25\n",
    "        # print('\\nCaliper (radius) is: {:.4f}\\n'.format(caliper))\n",
    "\n",
    "        df_data = X\n",
    "        knn = NearestNeighbors(n_neighbors=10 , p = 2, radius=caliper)\n",
    "        knn.fit(df_data[['propensity_score_logit']].to_numpy())\n",
    "        \n",
    "        distances , indexes = knn.kneighbors(\n",
    "            df_data[['propensity_score_logit']].to_numpy(), \\\n",
    "            n_neighbors=10)\n",
    "        \n",
    "        edmonds_applied = create_maximum_branching_graph(df_data)\n",
    "        treatment_effect = []\n",
    "        for (u,v) in edmonds_applied.edges():\n",
    "            effect = (df_data['outcome'][u] - df_data['outcome'][v])/(df_data['treatment'][u] - df_data['treatment'][v])\n",
    "            treatment_effect.append(effect)\n",
    "        effects.append(treatment_effect)\n",
    "        \n",
    "    return effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/bvankooten/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "effects_2011 = null_model(full_dataset_confounders_2011, 'total_count', all_columns_to_drop_year, bins=[4,7,12], iterations=10)\n",
    "effects_2012 = null_model(full_dataset_confounders_2012, 'total_count', all_columns_to_drop_year, bins=[4,7,12], iterations=10)\n",
    "effects_2013 = null_model(full_dataset_confounders_2013, 'total_count', all_columns_to_drop_year, bins=[4,7,12], iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_averages(effects):\n",
    "    ate = 0\n",
    "    minimum = 0\n",
    "    percentile_25 = 0\n",
    "    percentile_75 = 0\n",
    "    maximum = 0\n",
    "    for eff in effects:\n",
    "        ate += np.mean(eff)\n",
    "        minimum += np.min(eff)\n",
    "        percentile_25 += np.percentile(eff,25)\n",
    "        percentile_75 += np.percentile(eff,75)\n",
    "        maximum += np.max(eff)\n",
    "    print('ATE:', ate/10, 'Min:', minimum/10,'25th %:', percentile_25/10, '75th %:', percentile_75/10, 'Max:', maximum/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n",
      "ATE: 7.728811473101918 Min: -536.2983123280869 25th %: -71.50712998435475 75th %: 85.9880156042402 Max: 584.4928193578542\n"
     ]
    }
   ],
   "source": [
    "print('2011')\n",
    "calculate_averages(effects_2011)\n",
    "print('2012')\n",
    "calculate_averages(effects_2012)\n",
    "print('2013')\n",
    "calculate_averages(effects_2013)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
